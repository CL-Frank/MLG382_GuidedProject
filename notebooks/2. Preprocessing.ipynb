{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0974493b",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e187ee",
   "metadata": {},
   "source": [
    "## Getting The System Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0374fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import scipy.stats as stats\n",
    "# import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34dd433",
   "metadata": {},
   "source": [
    "## Loading The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the dataset\n",
    "file_path = \"../data/Student_performance_data .csv\" \n",
    "\n",
    "# Checking of the file\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"The file '{file_path}' was not found in {os.getcwd()}. \"\n",
    "        f\"Please ensure the file is in the correct directory or provide the correct path. \"\n",
    "        f\"Available files in 'data': {os.listdir('data')}\"\n",
    "    )\n",
    "    \n",
    "    # Loading the dataset\n",
    "try:\n",
    "    data = pd.read_csv(file_path)\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Failed to load CSV file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b03d8b",
   "metadata": {},
   "source": [
    "## 1. Missing Value and Oulier Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19f8f2",
   "metadata": {},
   "source": [
    "### *i.* Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any missing values of the dataset\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"\\nMissing Values:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aeab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure the numerical columns are numeric\n",
    "numerical_cols = ['StudyTimeWeekly', 'Absences', 'GPA']\n",
    "for col in numerical_cols:\n",
    "    try:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not convert {col} to numeric: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4341042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values (if there is any)\n",
    "for col in numerical_cols:\n",
    "    try:\n",
    "        if data[col].isnull().any():\n",
    "            median_value = data[col].median()\n",
    "            data[col] = data[col].fillna(median_value)\n",
    "            print(f\"Imputed missing values in {col} with median: {median_value}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error imputing {col}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2538ad",
   "metadata": {},
   "source": [
    "### *i.* Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c6239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check for any missing values in categorical columns, impute with mode\n",
    "categorical_cols = ['Gender', 'Ethnicity', 'ParentalEducation', 'Tutoring', 'ParentalSupport', \n",
    "                    'Extracurricular', 'Sports', 'Music', 'Volunteering']\n",
    "for col in categorical_cols:\n",
    "    try:\n",
    "        if data[col].isnull().any():\n",
    "            mode_value = data[col].mode()[0]\n",
    "            data[col] = data[col].fillna(mode_value)\n",
    "            print(f\"Imputed missing values in {col} with mode: {mode_value}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error imputing {col}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae07f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection and treatment using Z-score\n",
    "def detect_outliers_zscore(df, column, threshold=3):\n",
    "    try:\n",
    "        # Ensure column is numeric and drop NaN values for Z-score calculation\n",
    "        col_data = pd.to_numeric(df[column], errors='coerce').dropna()\n",
    "        if col_data.empty:\n",
    "            print(f\"No valid data in {column} for outlier detection\")\n",
    "            return pd.DataFrame()\n",
    "        z_scores = np.abs(stats.zscore(col_data))\n",
    "        outliers = df.loc[col_data.index][z_scores > threshold]\n",
    "        return outliers\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting outliers in {column}: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns to check for outliers\n",
    "for col in numerical_cols:\n",
    "    try:\n",
    "        outliers = detect_outliers_zscore(data, col)\n",
    "        if not outliers.empty:\n",
    "            print(f\"\\nOutliers in {col}:\\n\", outliers[[col]])\n",
    "        else:\n",
    "            print(f\"\\nNo outliers detected in {col}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing outliers for {col}: {e}\")\n",
    "\n",
    "# Capping the outliers at the 1st and 99th percentiles\n",
    "for col in numerical_cols:\n",
    "    try:\n",
    "        lower_bound = data[col].quantile(0.01)\n",
    "        upper_bound = data[col].quantile(0.99)\n",
    "        if not pd.isna(lower_bound) and not pd.isna(upper_bound):\n",
    "            data[col] = data[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "            print(f\"Capped outliers in {col} at 1st ({lower_bound}) and 99th ({upper_bound}) percentiles\")\n",
    "        else:\n",
    "            print(f\"Skipping outlier capping for {col} due to invalid quantiles\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error capping outliers in {col}: {e}\")\n",
    "\n",
    "# Verifying for the outlier treatment\n",
    "for col in numerical_cols:\n",
    "    try:\n",
    "        print(f\"\\nSummary of {col} after capping:\\n\", data[col].describe())\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing {col}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ac782",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50af238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new features\n",
    "data['StudyTimePerAbsence'] = data['StudyTimeWeekly'] / (data['Absences'] + 1)\n",
    "\n",
    "data['TotalExtracurricular'] = data[['Extracurricular', 'Sports', 'Music', 'Volunteering']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b71999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StudyTimeWeekly to be divided into categories\n",
    "bins = [0, 5, 10, 15, 20]\n",
    "labels = ['Low', 'Moderate', 'High', 'Very High']\n",
    "data['StudyTimeCategory'] = pd.cut(data['StudyTimeWeekly'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "categorical_cols_to_encode = ['Ethnicity', 'ParentalEducation', 'StudyTimeCategory']\n",
    "data_encoded = pd.get_dummies(data, columns=categorical_cols_to_encode, drop_first=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_cols_to_scale = ['Age','StudyTimeWeekly', 'Absences', 'GPA', 'StudyTimePerAbsence', 'TotalExtracurricular']\n",
    "scaler.fit_transform(data_encoded[numerical_cols_to_scale])\n",
    "data_encoded[numerical_cols_to_scale] = scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bc985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns of the dataset that are unnecessary\n",
    "data_encoded.drop(['StudentID', 'GradeClass'], axis=1, inplace=True) \n",
    "\n",
    "# For the final dataset to display\n",
    "print(\"\\nFinal Feature Set Columns:\\n\", data_encoded.columns)\n",
    "print(\"\\nFirst 5 rows of processed dataset:\\n\", data_encoded.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af9f907",
   "metadata": {},
   "source": [
    "## 3. Saving The Newly Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This saves the new processed dataset\n",
    "data_encoded.to_csv(\"../data/Processed_Student_Performance.csv\", index=False)\n",
    "print(\"\\nProcessed dataset saved as 'Processed_Student_Performance.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d72825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the new features and scaler\n",
    "with open(\"../artifacts/features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data_encoded.columns.tolist(), f)\n",
    "\n",
    "with open(\"../artifacts/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
